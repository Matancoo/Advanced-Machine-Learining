
**Week 4: Variational Models: Limitations of Auto-Regressive (AR) Models**

  - Last week we discussed auto-regressive models. They perform well for shorter sequences like text. However, they have a few limitations:
  - Inference runtime is linear in sequence length (O(n)).
  - They require some sequence ordering.
  - They fail to exploit low rank (discussed in the next slide).

**Illustration - Low-Rank Data: Low-Rank Manifold Assumption**

- Often, the data we encounter in the world is high-dimensional.
- However, it's usually generated by a small number of variation factors.
- When these factors are unknown, they're referred to as hidden or latent.

**Generative Model Objective**

- Suppose we have a set of training data (e.g., images) x1, x2...xN.
- Our aim is to learn a function p(x) that:
  - Gives high likelihood (p(x)) to the train/test images.
  - Is easy to sample from.
  - Can be used to compute point estimates.
- This is similar to the objective from the last week.

**Latent Variable Representation**

- A simple model for this data represents it by a hidden (latent) variable z, typically of low dimension.
- The latent variable follows a simple distribution p(z), such as Uniform, Gaussian, or Multinomial.
- This code, p(z), is a compact representation of p(x).

**Generator**

- A latent variable z is translated into a distribution over images x using function G.
- Intuitively, G(z) is the most likely image given the latent code z.
- These images are typically of much higher dimension than z.
- This function can be complex (e.g., a transformer).

**Posterior**

- Our goal is to have a probabilistic model.
- We model the probability of image x given code z.
- Here we use a simple Gaussian PDF.

**Likelihood**

- The distribution of x under our model.
- It's a function of the prior and posterior.

**Two Challenges**

- Efficiently compute the value of p(x) given x.
- Find the optimal parameters such that the data is most likely under the model (max-likelihood).

**Preliminary: Importance Sampling & Monte Carlo Estimation**

- Computing the integral may be challenging.
- We can adopt a sampling-based approach instead, sampling N values from p(x): x1,x2...xN, and estimate the average value of f on samples from p.
- However, several problems may occur, such as not knowing how to sample from p or needing too many samples.

**Proposal Distribution**

- We propose another probability distribution Q(x) such that it's easy to sample from and has a high probability at high values of p(x)f(x).
- The question arises if we can estimate expectation with respect to Q instead of P.

**Importance Sampling for Inference**

- How to select the best proposal distribution Q for image x?
- The idea is to optimize it!
- In fact, if we can select any q, equality follows.
- Let us substitute q(z) = p(z|x) - also known as the posterior.
- No proposal is better than the posterior. However, it's typically infeasible to compute. Direct optimization of q(z) is therefore the practical path.

**Variational Approximation in Practice**

- In practice, we want q(z) to be easy to sample from.
- Let's choose a Gaussian with independent dimensions.
- Each dimension d has mean and covariance.
- Optimization equates to finding the optimal set of mean and covariance for all d.

**Estimation of Likelihood of Entire Dataset**

- We may want to estimate the likelihood of multiple images x1...xN.
- For each image xi, we find the optimal proposal distribution qi.
- The log-likelihood of the entire dataset becomes a sum of individual log-likelihoods.
  
**Maximum-Likelihood (ML) Estimation of G**

- We seek a generator G under which the observed data is most likely.
- We therefore jointly optimize the parameters of G and of q1...qN.

**Re-Parameterization Trick, Latent Optimization, and Amortized Inference**

- Breakthrough work of Kingma and Welling proposed to replace the per-sample qi by a learned encoder. This formalism is called the Variational Autoencoder (VAE).
  
**VAE Optimization in Practice**

  - To simplify the maths, we often:
  - Choose p(z) = N(0,I).
  - Estimate the expectation using a single normal sample.
  
**Recap: Amortized Variational Inference**

- To find the best generator G, we need to estimate data likelihood.
- Simply taking expectation over z is too inefficient.
- We learn the best proposal distribution q for each x.
- Instead of optimizing q for each x, we train an encoder.
  
**Using VAE for Point Estimation**

- VAE can be used for computing p(x). We don't have to make the variational approximation. About 100 samples of z are recommended for good accuracy.
  
**Conditional VAE (cVAE) and Summary**

- cVAE Loss is very similar to non-conditional loss.
- Some datasets are well described by low-rank latent variables.
- Variational inference allows likelihood calculation.
- VAE proposed to amortize the proposal distributions.
- Conditional
