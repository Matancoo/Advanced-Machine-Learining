** QUESTIONS:**

- What are the major limitations of auto-regressive models?

- Can you explain the concept of "Low-Rank Manifold Assumption" and how it relates to high-dimensional data?

- What are the three characteristics we want our function p(x) to possess in the context of a set of training data?

- Explain the role of the latent variable z in the representation of data.

- How does a generator function G transform a latent variable z into a distribution over images x?

- What are the two main challenges in modeling the distribution of x under our model?

- What are the problems we might encounter when trying to implement importance sampling and Monte Carlo estimation?

- Why might it be beneficial to propose another probability distribution Q(x) for the sampling process?

- Why is the posterior typically considered the best proposal distribution?

- Why is it practically infeasible to compute the posterior?

- Explain the process of variational approximation in practice. Why do we typically choose a Gaussian?

- How do we estimate the likelihood of an entire dataset?

- What is the goal of maximum-likelihood (ML) estimation of G?

- Can you elaborate on the re-parameterization trick and the concept of amortized inference?

- What are the typical choices made to simplify VAE optimization in practice?

- Why do we need to estimate data likelihood to find the best generator G?-

- How can we use VAE for point estimation?

- What is the key difference between a Conditional VAE (cVAE) and a standard VAE?

- In summary, what are the main advantages of variational inference and the use of VAEs?


**Week 4: Variational Models: Limitations of Auto-Regressive (AR) Models**

  - Last week we discussed auto-regressive models. They perform well for shorter sequences like text. However, they have a few limitations:
  - Inference runtime is linear in sequence length (O(n)).
  - They require some sequence ordering.
  - They fail to exploit low rank (discussed in the next slide).

**Illustration - Low-Rank Data: Low-Rank Manifold Assumption**

- Often, the data we encounter in the world is high-dimensional.
- However, it's usually generated by a small number of variation factors.
- When these factors are unknown, they're referred to as hidden or latent.

**Generative Model Objective**

- Suppose we have a set of training data (e.g., images) x1, x2...xN.
- Our aim is to learn a function p(x) that:
  - Gives high likelihood (p(x)) to the train/test images.
  - Is easy to sample from.
  - Can be used to compute point estimates.
- This is similar to the objective from the last week.

**Latent Variable Representation**

- A simple model for this data represents it by a hidden (latent) variable z, typically of low dimension.
- The latent variable follows a simple distribution p(z), such as Uniform, Gaussian, or Multinomial.
- This code, p(z), is a compact representation of p(x).

**Generator**

- A latent variable z is translated into a distribution over images x using function G.
- Intuitively, G(z) is the most likely image given the latent code z.
- These images are typically of much higher dimension than z.
- This function can be complex (e.g., a transformer).

**Posterior**

- Our goal is to have a probabilistic model.
- We model the probability of image x given code z.
- Here we use a simple Gaussian PDF.

**Likelihood**

- The distribution of x under our model.
- It's a function of the prior and posterior.

**Two Challenges**

- Efficiently compute the value of p(x) given x.
- Find the optimal parameters such that the data is most likely under the model (max-likelihood).

**Preliminary: Importance Sampling & Monte Carlo Estimation**

- Computing the integral may be challenging.
- We can adopt a sampling-based approach instead, sampling N values from p(x): x1,x2...xN, and estimate the average value of f on samples from p.
- However, several problems may occur, such as not knowing how to sample from p or needing too many samples.

**Proposal Distribution**

- We propose another probability distribution Q(x) such that it's easy to sample from and has a high probability at high values of p(x)f(x).
- The question arises if we can estimate expectation with respect to Q instead of P.

**Importance Sampling for Inference**

- How to select the best proposal distribution Q for image x?
- The idea is to optimize it!
- In fact, if we can select any q, equality follows.
- Let us substitute q(z) = p(z|x) - also known as the posterior.
- No proposal is better than the posterior. However, it's typically infeasible to compute. Direct optimization of q(z) is therefore the practical path.

**Variational Approximation in Practice**

- In practice, we want q(z) to be easy to sample from.
- Let's choose a Gaussian with independent dimensions.
- Each dimension d has mean and covariance.
- Optimization equates to finding the optimal set of mean and covariance for all d.

**Estimation of Likelihood of Entire Dataset**

- We may want to estimate the likelihood of multiple images x1...xN.
- For each image xi, we find the optimal proposal distribution qi.
- The log-likelihood of the entire dataset becomes a sum of individual log-likelihoods.
  
**Maximum-Likelihood (ML) Estimation of G**

- We seek a generator G under which the observed data is most likely.
- We therefore jointly optimize the parameters of G and of q1...qN.

**Re-Parameterization Trick, Latent Optimization, and Amortized Inference**

- Breakthrough work of Kingma and Welling proposed to replace the per-sample qi by a learned encoder. This formalism is called the Variational Autoencoder (VAE).
  
**VAE Optimization in Practice**

  - To simplify the maths, we often:
  - Choose p(z) = N(0,I).
  - Estimate the expectation using a single normal sample.
  
**Recap: Amortized Variational Inference**

- To find the best generator G, we need to estimate data likelihood.
- Simply taking expectation over z is too inefficient.
- We learn the best proposal distribution q for each x.
- Instead of optimizing q for each x, we train an encoder.
  
**Using VAE for Point Estimation**

- VAE can be used for computing p(x). We don't have to make the variational approximation. About 100 samples of z are recommended for good accuracy.
  
**Conditional VAE (cVAE) and Summary**

- cVAE Loss is very similar to non-conditional loss.
- Some datasets are well described by low-rank latent variables.
- Variational inference allows likelihood calculation.
- VAE proposed to amortize the proposal distributions.
- Conditional



** ANSWERS:**
The major limitations of auto-regressive models include linear inference runtime in sequence length, the requirement of some ordering, and inability to take advantage of low rank.

The "Low-Rank Manifold Assumption" suggests that while our observed data may be high-dimensional, it's often generated by a smaller number of factors of variation. These unknown factors are referred to as hidden or latent.

We want our function p(x) to ensure that the train/test images have a high likelihood (p(x)), it is easy to sample from it, and we can use it to compute point estimates.

The latent variable z serves as a compact, typically low-dimensional, representation of the high-dimensional data x. Its simple distribution p(z) aids in modeling the data.

The generator function G maps the latent variable z to a distribution over images x. Intuitively, G(z) denotes the most likely image given the latent code z.

The two main challenges are efficiently computing the value of p(x|z) given x and finding optimal parameters to maximize the likelihood of the data under the model.

Importance sampling and Monte Carlo estimation might face difficulties such as inability to sample from p, or the requirement of too many samples which can be computationally expensive.

Proposing another probability distribution Q(x) could make the sampling process easier, especially when the original distribution p(x) is complex or unknown.

The posterior is considered the best proposal distribution because it maximizes the expectation of the target function.

Computing the posterior is usually computationally complex and therefore infeasible, requiring us to resort to approximations.

Variational approximation involves selecting a proposal distribution that is easy to sample from, often a Gaussian. Optimization involves finding the optimal parameters for this distribution.

We estimate the likelihood of an entire dataset by finding the optimal proposal distribution for each image and estimating log likelihood.

ML estimation seeks the generator G that makes the observed data most likely. This involves jointly optimizing the parameters of G and the proposal distributions.

The re-parameterization trick allows us to backpropagate through random nodes, and amortized inference involves learning a single encoder across all samples instead of per-sample proposal distributions.

To simplify VAE optimization in practice, we typically choose a standard Gaussian for p(z), and use a single sample to estimate the expectation.

Estimating data likelihood is crucial to finding the best generator G because it provides a measure of how well our model represents the training data.

VAE can be used for point estimation by calculating the expectation of log p(x|z) with respect to q(z|x).

A cVAE is an extension of the standard VAE where the latent variables and the data are conditioned on additional variables, allowing more control over the data generation process.

Variational inference enables efficient likelihood calculation and allows for the training of better data generators. VAEs further enhance this process by amortizing the proposal distributions.
